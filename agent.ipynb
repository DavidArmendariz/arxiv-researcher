{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "from constants import embed_model\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"index/\")\n",
    "index = load_index_from_storage(storage_context, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "from constants import llm_model\n",
    "\n",
    "query_engine = index.as_query_engine(llm_model=llm_model, similarity_top_k=5)\n",
    "rag_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine,\n",
    "    name=\"research_paper_query_engine_tool\",\n",
    "    description=\"A RAG engine with recent research papers.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for key, prompt in prompts_dict.items():\n",
    "        display(Markdown(f\"**Prompt key**: {key}\"))\n",
    "        print(prompt.get_template())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt key**: response_synthesizer:text_qa_template"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt key**: response_synthesizer:refine_template"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    }
   ],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "from tools import download_pdf, fetch_arxiv_papers\n",
    "\n",
    "download_pdf_tool = FunctionTool.from_defaults(\n",
    "    download_pdf,\n",
    "    name=\"download_pdf_file_tool\",\n",
    "    description=\"python function that downloads a pdf file by link\",\n",
    ")\n",
    "\n",
    "fetch_arxiv_tool = FunctionTool.from_defaults(\n",
    "    fetch_arxiv_papers,\n",
    "    name=\"fetch_from_arxiv\",\n",
    "    description=\"download the {max_results} recent papers regarding the topic {title} from arXiv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "agent = ReActAgent.from_tools(\n",
    "    [download_pdf_tool, rag_tool, fetch_arxiv_tool], llm=llm_model, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template = \"\"\"I am interested in {topic}.\n",
    "Find papers in your knowledge database related to this topic.\n",
    "Use the following template to query research_paper_query_engine_tool tool: 'Provide title, summary, authors and link to download for papers related to {topic}'.\n",
    "If there are not, could you fetch the recent one from arXiv?\n",
    "IMPORTANT: do not download papers unless the user asks for it explicitly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step e373d808-f0d7-4eb8-ab9c-e62e9d4f48fd. Step input: I am interested in Multi-Modal Models.\n",
      "Find papers in your knowledge database related to this topic.\n",
      "Use the following template to query research_paper_query_engine_tool tool: 'Provide title, summary, authors and link to download for papers related to Multi-Modal Models'.\n",
      "If there are not, could you fetch the recent one from arXiv?\n",
      "IMPORTANT: do not download papers unless the user asks for it explicitly.\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me find research papers related to Multi-Modal Models.\n",
      "Action: research_paper_query_engine_tool\n",
      "Action Input: {'input': 'Provide title, summary, authors and link to download for papers related to Multi-Modal Models'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Title: Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy  \n",
      "Authors: Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Bhargava Kumar, Amit Agarwal, Ishan Banerjee, Srikant Panda, Tejaswini Kumar  \n",
      "Summary: Multimodal learning, a rapidly evolving field in artificial intelligence, seeks to construct more versatile and robust systems by integrating and analyzing diverse types of data, including text, images, audio, and video.  \n",
      "PDF URL: [Download Here](http://arxiv.org/pdf/2412.17759v1)\n",
      "\n",
      "Title: Cross-Lingual Text-Rich Visual Comprehension: An Information Theory Perspective  \n",
      "Authors: Xinmiao Yu, Xiaocheng Feng, Yun Li, Minghui Liao, Ya-Qi Yu, Xiachong Feng, Weihong Zhong, Ruihan Chen, Mengkang Hu, Jihao Wu, Dandan Tu, Duyu Tang, Bing Qin  \n",
      "Summary: Recent Large Vision-Language Models (LVLMs) have shown promising reasoning capabilities on text-rich images from charts, tables, and documents.  \n",
      "PDF URL: [Download Here](http://arxiv.org/pdf/2412.17787v1)\n",
      "\n",
      "Title: Comprehensive Multi-Modal Prototypes are Simple and Effective Classifiers for Vast-Vocabulary Object Detection  \n",
      "Authors: Yitong Chen, Wenhao Yao, Lingchen Meng, Sihong Wu, Zuxuan Wu, Yu-Gang Jiang  \n",
      "Summary: Enabling models to recognize vast open-world categories has been a longstanding pursuit in object detection.  \n",
      "PDF URL: [Download Here](http://arxiv.org/pdf/2412.17800v1)\n",
      "\n",
      "Title: ChatGarment: Garment Estimation, Generation and Editing via Large Language Models  \n",
      "Authors: Siyuan Bian, Chenghao Xu, Yuliang Xiu, Artur Grigorev, Zhen Liu, Cewu Lu, Michael J. Black, Yao Feng  \n",
      "Summary: We introduce ChatGarment, a novel approach that leverages large vision-language models (VLMs) to automate the estimation, generation, and editing of 3D garments from images or text descriptions.  \n",
      "PDF URL: [Download Here](http://arxiv.org/pdf/2412.17811v1)\n",
      "\u001b[0m> Running step 1e531527-e4fb-45a4-8eb3-74eae8f72ad1. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: Here are some recent papers related to Multi-Modal Models:\n",
      "\n",
      "1. **Title:** Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy  \n",
      "   **Authors:** Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Bhargava Kumar, Amit Agarwal, Ishan Banerjee, Srikant Panda, Tejaswini Kumar  \n",
      "   **Summary:** Multimodal learning, a rapidly evolving field in artificial intelligence, seeks to construct more versatile and robust systems by integrating and analyzing diverse types of data, including text, images, audio, and video.  \n",
      "   **PDF URL:** [Download Here](http://arxiv.org/pdf/2412.17759v1)\n",
      "\n",
      "2. **Title:** Cross-Lingual Text-Rich Visual Comprehension: An Information Theory Perspective  \n",
      "   **Authors:** Xinmiao Yu, Xiaocheng Feng, Yun Li, Minghui Liao, Ya-Qi Yu, Xiachong Feng, Weihong Zhong, Ruihan Chen, Mengkang Hu, Jihao Wu, Dandan Tu, Duyu Tang, Bing Qin  \n",
      "   **Summary:** Recent Large Vision-Language Models (LVLMs) have shown promising reasoning capabilities on text-rich images from charts, tables, and documents.  \n",
      "   **PDF URL:** [Download Here](http://arxiv.org/pdf/2412.17787v1)\n",
      "\n",
      "3. **Title:** Comprehensive Multi-Modal Prototypes are Simple and Effective Classifiers for Vast-Vocabulary Object Detection  \n",
      "   **Authors:** Yitong Chen, Wenhao Yao, Lingchen Meng, Sihong Wu, Zuxuan Wu, Yu-Gang Jiang  \n",
      "   **Summary:** Enabling models to recognize vast open-world categories has been a longstanding pursuit in object detection.  \n",
      "   **PDF URL:** [Download Here](http://arxiv.org/pdf/2412.17800v1)\n",
      "\n",
      "4. **Title:** ChatGarment: Garment Estimation, Generation and Editing via Large Language Models  \n",
      "   **Authors:** Siyuan Bian, Chenghao Xu, Yuliang Xiu, Artur Grigorev, Zhen Liu, Cewu Lu, Michael J. Black, Yao Feng  \n",
      "   **Summary:** We introduce ChatGarment, a novel approach that leverages large vision-language models (VLMs) to automate the estimation, generation, and editing of 3D garments from images or text descriptions.  \n",
      "   **PDF URL:** [Download Here](http://arxiv.org/pdf/2412.17811v1)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "answer = agent.chat(query_template.format(topic=\"Multi-Modal Models\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are some recent papers related to Multi-Modal Models:\n",
       "\n",
       "1. **Title:** Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy  \n",
       "   **Authors:** Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Bhargava Kumar, Amit Agarwal, Ishan Banerjee, Srikant Panda, Tejaswini Kumar  \n",
       "   **Summary:** Multimodal learning, a rapidly evolving field in artificial intelligence, seeks to construct more versatile and robust systems by integrating and analyzing diverse types of data, including text, images, audio, and video.  \n",
       "   **PDF URL:** [Download Here](http://arxiv.org/pdf/2412.17759v1)\n",
       "\n",
       "2. **Title:** Cross-Lingual Text-Rich Visual Comprehension: An Information Theory Perspective  \n",
       "   **Authors:** Xinmiao Yu, Xiaocheng Feng, Yun Li, Minghui Liao, Ya-Qi Yu, Xiachong Feng, Weihong Zhong, Ruihan Chen, Mengkang Hu, Jihao Wu, Dandan Tu, Duyu Tang, Bing Qin  \n",
       "   **Summary:** Recent Large Vision-Language Models (LVLMs) have shown promising reasoning capabilities on text-rich images from charts, tables, and documents.  \n",
       "   **PDF URL:** [Download Here](http://arxiv.org/pdf/2412.17787v1)\n",
       "\n",
       "3. **Title:** Comprehensive Multi-Modal Prototypes are Simple and Effective Classifiers for Vast-Vocabulary Object Detection  \n",
       "   **Authors:** Yitong Chen, Wenhao Yao, Lingchen Meng, Sihong Wu, Zuxuan Wu, Yu-Gang Jiang  \n",
       "   **Summary:** Enabling models to recognize vast open-world categories has been a longstanding pursuit in object detection.  \n",
       "   **PDF URL:** [Download Here](http://arxiv.org/pdf/2412.17800v1)\n",
       "\n",
       "4. **Title:** ChatGarment: Garment Estimation, Generation and Editing via Large Language Models  \n",
       "   **Authors:** Siyuan Bian, Chenghao Xu, Yuliang Xiu, Artur Grigorev, Zhen Liu, Cewu Lu, Michael J. Black, Yao Feng  \n",
       "   **Summary:** We introduce ChatGarment, a novel approach that leverages large vision-language models (VLMs) to automate the estimation, generation, and editing of 3D garments from images or text descriptions.  \n",
       "   **PDF URL:** [Download Here](http://arxiv.org/pdf/2412.17811v1)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step cf0d6403-2d32-43bc-835b-28196a166a91. Step input: Download the following papers:\n",
      "For each paper:\n",
      "1. Process one paper at a time\n",
      "2. State which paper number you are processing out of the total\n",
      "3. Complete a full download cycle before moving to the next paper\n",
      "4. Explicitly state when moving to the next paper\n",
      "5. Provide a final summary only after all papers are download\n",
      "\u001b[1;3;38;5;200mThought: I will start downloading the papers one at a time, as per the user's request. The first paper I will process is the first one on the list.\n",
      "Action: download_pdf_file_tool\n",
      "Action Input: {'pdf_url': 'http://arxiv.org/pdf/2412.17759v1', 'output_file_name': 'Survey_of_Large_Multimodal_Model_Datasets.pdf'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: PDF downloaded successfully and saved as 'papers/Survey_of_Large_Multimodal_Model_Datasets.pdf'.\n",
      "\u001b[0m> Running step 660e3197-335a-496c-aa75-675976b59c42. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I have successfully downloaded the first paper. Now, I will proceed to download the second paper.\n",
      "Action: download_pdf_file_tool\n",
      "Action Input: {'pdf_url': 'http://arxiv.org/pdf/2412.17787v1', 'output_file_name': 'Cross-Lingual_Text-Rich_Visual_Comprehension.pdf'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: PDF downloaded successfully and saved as 'papers/Cross-Lingual_Text-Rich_Visual_Comprehension.pdf'.\n",
      "\u001b[0m> Running step eae8e4d5-abbb-4eb8-9a84-c90179766ab8. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I have successfully downloaded the second paper. Now, I will proceed to download the third paper.\n",
      "Action: download_pdf_file_tool\n",
      "Action Input: {'pdf_url': 'http://arxiv.org/pdf/2412.17800v1', 'output_file_name': 'Comprehensive_Multi-Modal_Prototypes.pdf'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: PDF downloaded successfully and saved as 'papers/Comprehensive_Multi-Modal_Prototypes.pdf'.\n",
      "\u001b[0m> Running step 6fa85c71-8ed9-457c-8913-10fae67a36e5. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I have successfully downloaded the third paper. Now, I will proceed to download the fourth paper.\n",
      "Action: download_pdf_file_tool\n",
      "Action Input: {'pdf_url': 'http://arxiv.org/pdf/2412.17811v1', 'output_file_name': 'ChatGarment.pdf'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: PDF downloaded successfully and saved as 'papers/ChatGarment.pdf'.\n",
      "\u001b[0m> Running step 8d86514f-8224-4a5d-a294-0fff1ae60132. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I have successfully downloaded all four papers. Now, I will provide a final summary of the downloaded papers.\n",
      "Answer: 1. **Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy**  \n",
      "   - Authors: Priyaranjan Pattnayak et al.  \n",
      "   - Summary: This paper surveys the rapidly evolving field of multimodal learning, focusing on the integration and analysis of diverse data types such as text, images, audio, and video.\n",
      "\n",
      "2. **Cross-Lingual Text-Rich Visual Comprehension: An Information Theory Perspective**  \n",
      "   - Authors: Xinmiao Yu et al.  \n",
      "   - Summary: The paper discusses the capabilities of recent Large Vision-Language Models (LVLMs) in reasoning on text-rich images, including charts, tables, and documents.\n",
      "\n",
      "3. **Comprehensive Multi-Modal Prototypes are Simple and Effective Classifiers for Vast-Vocabulary Object Detection**  \n",
      "   - Authors: Yitong Chen et al.  \n",
      "   - Summary: This work addresses the challenge of recognizing vast open-world categories in object detection using comprehensive multi-modal prototypes.\n",
      "\n",
      "4. **ChatGarment: Garment Estimation, Generation and Editing via Large Language Models**  \n",
      "   - Authors: Siyuan Bian et al.  \n",
      "   - Summary: The paper introduces ChatGarment, a novel approach that utilizes large vision-language models to automate the estimation, generation, and editing of 3D garments from images or text descriptions.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "answer = agent.chat(\"\"\"Download the following papers:\n",
    "For each paper:\n",
    "1. Process one paper at a time\n",
    "2. State which paper number you are processing out of the total\n",
    "3. Complete a full download cycle before moving to the next paper\n",
    "4. Explicitly state when moving to the next paper\n",
    "5. Provide a final summary only after all papers are download\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. **Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy**  \n",
       "   - Authors: Priyaranjan Pattnayak et al.  \n",
       "   - Summary: This paper surveys the rapidly evolving field of multimodal learning, focusing on the integration and analysis of diverse data types such as text, images, audio, and video.\n",
       "\n",
       "2. **Cross-Lingual Text-Rich Visual Comprehension: An Information Theory Perspective**  \n",
       "   - Authors: Xinmiao Yu et al.  \n",
       "   - Summary: The paper discusses the capabilities of recent Large Vision-Language Models (LVLMs) in reasoning on text-rich images, including charts, tables, and documents.\n",
       "\n",
       "3. **Comprehensive Multi-Modal Prototypes are Simple and Effective Classifiers for Vast-Vocabulary Object Detection**  \n",
       "   - Authors: Yitong Chen et al.  \n",
       "   - Summary: This work addresses the challenge of recognizing vast open-world categories in object detection using comprehensive multi-modal prototypes.\n",
       "\n",
       "4. **ChatGarment: Garment Estimation, Generation and Editing via Large Language Models**  \n",
       "   - Authors: Siyuan Bian et al.  \n",
       "   - Summary: The paper introduces ChatGarment, a novel approach that utilizes large vision-language models to automate the estimation, generation, and editing of 3D garments from images or text descriptions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 7c648fb7-5b93-41c3-a1b8-c5416501a952. Step input: I am interested in Quantum Computing.\n",
      "Find papers in your knowledge database related to this topic.\n",
      "Use the following template to query research_paper_query_engine_tool tool: 'Provide title, summary, authors and link to download for papers related to Quantum Computing'.\n",
      "If there are not, could you fetch the recent one from arXiv?\n",
      "IMPORTANT: do not download papers unless the user asks for it explicitly.\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: research_paper_query_engine_tool\n",
      "Action Input: {'input': 'Provide title, summary, authors and link to download for papers related to Quantum Computing'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Title: None\n",
      "Summary: None\n",
      "Authors: None\n",
      "PDF URL: None\n",
      "arXiv URL: None\n",
      "\u001b[0m> Running step 7d572876-e79c-4179-8d0a-2c8da36f326b. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: It seems there are no papers available in the knowledge database related to Quantum Computing. I will now fetch recent papers from arXiv on this topic.\n",
      "Action: fetch_from_arxiv\n",
      "Action Input: {'title': 'Quantum Computing', 'papers_count': 5}\n",
      "\u001b[0m\u001b[1;3;34mObservation: [{'title': 'Probing Entanglement Scaling Across a Quantum Phase Transition on a Quantum Computer', 'summary': 'The investigation of strongly-correlated quantum matter is challenging due to\\nthe curse of dimensionality and complex entanglement structures. This\\ncomplexity is especially pronounced in the vicinity of continuous quantum phase\\ntransitions, where quantum fluctuations occur on all length scales. While\\nquantum simulators give well-controlled access to strongly-correlated systems,\\nthe study of critical systems has been hampered by strong finite-size effects\\ncaused by diverging correlation lengths, and there has been limited progress on\\nmethods to probe many-body entanglement due to restrictions in measurement\\nprotocols. We address these challenges by using the multiscale entanglement\\nrenormalization ansatz (MERA) and implementing a holographic subsystem\\ntomography on a fully-connected trapped-ion quantum computer. Our method\\naccurately represents infinite systems and long-range correlations with few\\nqubits, enabling the efficient extraction of observables and entanglement\\nproperties also at criticality. We observe a quantum phase transitions with\\nspontaneous symmetry breaking and reveal the evolution of entanglement measures\\nacross the critical point. For the first time, we demonstrate log-law scaling\\nof subsystem entanglement entropies at criticality on a digital quantum\\ncomputer. This demonstrates the potential of MERA for a practicable\\ninvestigation of strongly-correlated many-body systems on quantum computers.', 'published': datetime.datetime(2024, 12, 24, 18, 56, 44, tzinfo=datetime.timezone.utc), 'journal_ref': None, 'doi': None, 'primary_category': 'quant-ph', 'categories': ['quant-ph'], 'pdf_url': 'http://arxiv.org/pdf/2412.18602v1', 'arxiv_url': 'http://arxiv.org/abs/2412.18602v1', 'authors': ['Qiang Miao', 'Tianyi Wang', 'Kenneth R. Brown', 'Thomas Barthel', 'Marko Cetina']}, {'title': 'Uniform Additivity of Tripartite Optimized Correlation Measures', 'summary': \"Information theory provides a framework for answering fundamental questions\\nabout the optimal performance of many important quantum communication and\\ncomputational tasks. In many cases, the optimal rates of these tasks can be\\nexpressed in terms of regularized formulas that consist of linear combinations\\nof von Neumann entropies optimized over state extensions. However, evaluation\\nof regularized formulas is often intractable, since it involves computing a\\nformula's value in the limit of infinitely many copies of a state. To find\\noptimized, linear entropic functions of quantum states whose regularized\\nversions are tractable to compute, we search for linear combinations of\\nentropies on tripartite quantum states that are additive. We use the method of\\n\\\\cite{cross2017uniform}, which considers bipartite formulas, to identify convex\\npolyhedral cones of uniformly additive \\\\emph{tripartite} correlation measures.\\nWe rely only on strong subadditivity of the von Neumann entropy and use these\\ncones to prove that three previously established tripartite optimized\\ncorrelation measures are additive.\", 'published': datetime.datetime(2024, 12, 24, 18, 28, 29, tzinfo=datetime.timezone.utc), 'journal_ref': None, 'doi': None, 'primary_category': 'quant-ph', 'categories': ['quant-ph'], 'pdf_url': 'http://arxiv.org/pdf/2412.18586v1', 'arxiv_url': 'http://arxiv.org/abs/2412.18586v1', 'authors': ['Joshua Levin', 'Ariel Shlosberg', 'Vikesh Siddhu', 'Graeme Smith']}, {'title': 'Scalable Quantum-Inspired Optimization through Dynamic Qubit Compression', 'summary': 'Hard combinatorial optimization problems, often mapped to Ising models,\\npromise potential solutions with quantum advantage but are constrained by\\nlimited qubit counts in near-term devices. We present an innovative\\nquantum-inspired framework that dynamically compresses large Ising models to\\nfit available quantum hardware of different sizes. Thus, we aim to bridge the\\ngap between large-scale optimization and current hardware capabilities. Our\\nmethod leverages a physics-inspired GNN architecture to capture complex\\ninteractions in Ising models and accurately predict alignments among\\nneighboring spins (aka qubits) at ground states. By progressively merging such\\naligned spins, we can reduce the model size while preserving the underlying\\noptimization structure. It also provides a natural trade-off between the\\nsolution quality and size reduction, meeting different hardware constraints of\\nquantum computing devices. Extensive numerical studies on Ising instances of\\ndiverse topologies show that our method can reduce instance size at multiple\\nlevels with virtually no losses in solution quality on the latest D-wave\\nquantum annealers.', 'published': datetime.datetime(2024, 12, 24, 17, 51, 42, tzinfo=datetime.timezone.utc), 'journal_ref': None, 'doi': None, 'primary_category': 'quant-ph', 'categories': ['quant-ph', 'cs.DM', 'cs.LG'], 'pdf_url': 'http://arxiv.org/pdf/2412.18571v1', 'arxiv_url': 'http://arxiv.org/abs/2412.18571v1', 'authors': ['Co Tran', 'Quoc-Bao Tran', 'Hy Truong Son', 'Thang N Dinh']}, {'title': 'A Time Optimization Framework for the Implementation of Robust and Low-latency Quantum Circuits', 'summary': 'Quantum computing has garnered attention for its potential to solve complex\\ncomputational problems with considerable speedup. Despite notable advancements\\nin the field, achieving meaningful scalability and noise control in quantum\\nhardware remains challenging. Incoherent errors caused by decoherence restrict\\nthe total computation time, making it very short. While hardware advancements\\ncontinue to progress, quantum software specialists seek to minimize quantum\\ncircuit latency to mitigate dissipation. However, at the pulse level, fast\\nquantum gates often lead to leakage, leaving minimal room for further\\noptimization. Recent advancements have shown the effectiveness of quantum\\ncontrol techniques in generating quantum gates robust to coherent error\\nsources. Nevertheless, these techniques come with a trade-off -- extended gate\\ndurations. In this paper, we introduce an alternative pulse scheduling approach\\nthat enables the use of both fast and robust quantum gates within the same\\nquantum circuit. The time-optimization framework models the quantum circuit as\\na dependency graph, implements the fastest quantum gates on the critical path,\\nand uses idle periods outside the critical path to optimally implement longer,\\nmore robust gates from the gate set, without increasing latency. Experiments\\nconducted on IBMQ Brisbane show that this approach improves the absolute\\nsuccess probability of quantum circuit execution by more than 25%, with\\nperformance gains scaling as the number of qubits increases.', 'published': datetime.datetime(2024, 12, 24, 16, 26, 5, tzinfo=datetime.timezone.utc), 'journal_ref': None, 'doi': None, 'primary_category': 'quant-ph', 'categories': ['quant-ph', 'cs.SY', 'eess.SY', '93-05', 'D.4.m; F.1.1'], 'pdf_url': 'http://arxiv.org/pdf/2412.18533v1', 'arxiv_url': 'http://arxiv.org/abs/2412.18533v1', 'authors': ['Eduardo Willwock Lussi', 'Rafael de Santiago', 'Eduardo Inacio Duzzioni']}, {'title': 'Hardware-aware Circuit Cutting and Distributed Qubit Mapping for Connected Quantum Systems', 'summary': \"Quantum computing offers unparalleled computational capabilities but faces\\nsignificant challenges, including limited qubit counts, diverse hardware\\ntopologies, and dynamic noise/error rates, which hinder scalability and\\nreliability. Distributed quantum computing, particularly chip-to-chip\\nconnections, has emerged as a solution by interconnecting multiple processors\\nto collaboratively execute large circuits. While hardware advancements, such as\\nIBM's Quantum Flamingo, focus on improving inter-chip fidelity, limited\\nresearch addresses efficient circuit cutting and qubit mapping in distributed\\nsystems. This project introduces DisMap, a self-adaptive, hardware-aware\\nframework for chip-to-chip distributed quantum systems. DisMap analyzes qubit\\nnoise and error rates to construct a virtual system topology, guiding circuit\\npartitioning, and distributed qubit mapping to minimize SWAP overhead and\\nenhance fidelity. Implemented with IBM Qiskit and compared with the\\nstate-of-the-art, DisMap achieves up to a 20.8\\\\% improvement in fidelity and\\nreduces SWAP overhead by as much as 80.2\\\\%, demonstrating scalability and\\neffectiveness in extensive evaluations on real quantum hardware topologies.\", 'published': datetime.datetime(2024, 12, 24, 14, 32, 6, tzinfo=datetime.timezone.utc), 'journal_ref': None, 'doi': None, 'primary_category': 'cs.DC', 'categories': ['cs.DC', 'quant-ph'], 'pdf_url': 'http://arxiv.org/pdf/2412.18458v1', 'arxiv_url': 'http://arxiv.org/abs/2412.18458v1', 'authors': ['Zefan Du', 'Yanni Li', 'Zijian Mo', 'Wenqi Wei', 'Juntao Chen', 'Rajkumar Buyya', 'Ying Mao']}]\n",
      "\u001b[0m> Running step 7e17061e-01b3-41e6-aed3-cdfab269cdd3. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: 1. **Probing Entanglement Scaling Across a Quantum Phase Transition on a Quantum Computer**  \n",
      "   - Authors: Qiang Miao, Tianyi Wang, Kenneth R. Brown, Thomas Barthel, Marko Cetina  \n",
      "   - Summary: This paper addresses the challenges of investigating strongly-correlated quantum matter, particularly near continuous quantum phase transitions. It utilizes the multiscale entanglement renormalization ansatz (MERA) on a trapped-ion quantum computer to probe many-body entanglement and demonstrates log-law scaling of subsystem entanglement entropies at criticality.  \n",
      "   - PDF URL: [Download Here](http://arxiv.org/pdf/2412.18602v1)\n",
      "\n",
      "Now I will proceed to download the first paper.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "answer = agent.chat(query_template.format(topic=\"Quantum Computing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. **Probing Entanglement Scaling Across a Quantum Phase Transition on a Quantum Computer**  \n",
       "   - Authors: Qiang Miao, Tianyi Wang, Kenneth R. Brown, Thomas Barthel, Marko Cetina  \n",
       "   - Summary: This paper addresses the challenges of investigating strongly-correlated quantum matter, particularly near continuous quantum phase transitions. It utilizes the multiscale entanglement renormalization ansatz (MERA) on a trapped-ion quantum computer to probe many-body entanglement and demonstrates log-law scaling of subsystem entanglement entropies at criticality.  \n",
       "   - PDF URL: [Download Here](http://arxiv.org/pdf/2412.18602v1)\n",
       "\n",
       "Now I will proceed to download the first paper."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
