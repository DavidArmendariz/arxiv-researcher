{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import fetch_arxiv_papers\n",
    "\n",
    "papers = fetch_arxiv_papers(\"Language Models\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChatGarment: Garment Estimation, Generation and Editing via Large Language Models',\n",
       " 'Examining Imbalance Effects on Performance and Demographic Fairness of Clinical Language Models',\n",
       " 'Comprehensive Multi-Modal Prototypes are Simple and Effective Classifiers for Vast-Vocabulary Object Detection',\n",
       " 'Memory makes computation universal, remember?',\n",
       " 'Cross-Lingual Text-Rich Visual Comprehension: An Information Theory Perspective',\n",
       " 'PepTune: De Novo Generation of Therapeutic Peptides with Multi-Objective-Guided Discrete Diffusion',\n",
       " 'ResearchTown: Simulator of Human Research Community',\n",
       " 'Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy',\n",
       " 'ADC: Enhancing Function Calling Via Adversarial Datasets and Code Line-Level Feedback',\n",
       " 'Deliberation in Latent Space via Differentiable Cache Augmentation']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[paper[\"title\"] for paper in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "\n",
    "def create_documents_from_papers(papers):\n",
    "    documents = []\n",
    "    for paper in papers:\n",
    "        content = (\n",
    "            f\"Title: {paper['title']}\\n\"\n",
    "            f\"Authors: {', '.join(paper['authors'])}\\n\"\n",
    "            f\"Summary: {paper['summary']}\\n\"\n",
    "            f\"Published: {paper['published']}\\n\"\n",
    "            f\"Journal Reference: {paper['journal_ref']}\\n\"\n",
    "            f\"DOI: {paper['doi']}\\n\"\n",
    "            f\"Primary Category: {paper['primary_category']}\\n\"\n",
    "            f\"Categories: {', '.join(paper['categories'])}\\n\"\n",
    "            f\"PDF URL: {paper['pdf_url']}\\n\"\n",
    "            f\"arXiv URL: {paper['arxiv_url']}\\n\"\n",
    "        )\n",
    "        documents.append(Document(text=content))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = create_documents_from_papers(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='c7c54d4f-f645-445e-af2b-8b6c5d53de15', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: ChatGarment: Garment Estimation, Generation and Editing via Large Language Models\\nAuthors: Siyuan Bian, Chenghao Xu, Yuliang Xiu, Artur Grigorev, Zhen Liu, Cewu Lu, Michael J. Black, Yao Feng\\nSummary: We introduce ChatGarment, a novel approach that leverages large\\nvision-language models (VLMs) to automate the estimation, generation, and\\nediting of 3D garments from images or text descriptions. Unlike previous\\nmethods that struggle in real-world scenarios or lack interactive editing\\ncapabilities, ChatGarment can estimate sewing patterns from in-the-wild images\\nor sketches, generate them from text descriptions, and edit garments based on\\nuser instructions, all within an interactive dialogue. These sewing patterns\\ncan then be draped into 3D garments, which are easily animatable and\\nsimulatable. This is achieved by finetuning a VLM to directly generate a JSON\\nfile that includes both textual descriptions of garment types and styles, as\\nwell as continuous numerical attributes. This JSON file is then used to create\\nsewing patterns through a programming parametric model. To support this, we\\nrefine the existing programming model, GarmentCode, by expanding its garment\\ntype coverage and simplifying its structure for efficient VLM fine-tuning.\\nAdditionally, we construct a large-scale dataset of image-to-sewing-pattern and\\ntext-to-sewing-pattern pairs through an automated data pipeline. Extensive\\nevaluations demonstrate ChatGarment's ability to accurately reconstruct,\\ngenerate, and edit garments from multimodal inputs, highlighting its potential\\nto revolutionize workflows in fashion and gaming applications. Code and data\\nwill be available at https://chatgarment.github.io/.\\nPublished: 2024-12-23 18:59:28+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV\\nPDF URL: http://arxiv.org/pdf/2412.17811v1\\narXiv URL: http://arxiv.org/abs/2412.17811v1\\n\", mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cfedc644-33ce-4bc5-9ee4-8a4e313d6b9b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Examining Imbalance Effects on Performance and Demographic Fairness of Clinical Language Models\\nAuthors: Precious Jones, Weisi Liu, I-Chan Huang, Xiaolei Huang\\nSummary: Data imbalance is a fundamental challenge in applying language models to\\nbiomedical applications, particularly in ICD code prediction tasks where label\\nand demographic distributions are uneven. While state-of-the-art language\\nmodels have been increasingly adopted in biomedical tasks, few studies have\\nsystematically examined how data imbalance affects model performance and\\nfairness across demographic groups. This study fills the gap by statistically\\nprobing the relationship between data imbalance and model performance in ICD\\ncode prediction. We analyze imbalances in a standard benchmark data across\\ngender, age, ethnicity, and social determinants of health by state-of-the-art\\nbiomedical language models. By deploying diverse performance metrics and\\nstatistical analyses, we explore the influence of data imbalance on performance\\nvariations and demographic fairness. Our study shows that data imbalance\\nsignificantly impacts model performance and fairness, but feature similarity to\\nthe majority class may be a more critical factor. We believe this study\\nprovides valuable insights for developing more equitable and robust language\\nmodels in healthcare applications.\\nPublished: 2024-12-23 18:58:11+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.LG\\nCategories: cs.LG\\nPDF URL: http://arxiv.org/pdf/2412.17803v1\\narXiv URL: http://arxiv.org/abs/2412.17803v1\\n', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1173912e-56a7-4879-81de-3c15e0aab9db', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Comprehensive Multi-Modal Prototypes are Simple and Effective Classifiers for Vast-Vocabulary Object Detection\\nAuthors: Yitong Chen, Wenhao Yao, Lingchen Meng, Sihong Wu, Zuxuan Wu, Yu-Gang Jiang\\nSummary: Enabling models to recognize vast open-world categories has been a\\nlongstanding pursuit in object detection. By leveraging the generalization\\ncapabilities of vision-language models, current open-world detectors can\\nrecognize a broader range of vocabularies, despite being trained on limited\\ncategories. However, when the scale of the category vocabularies during\\ntraining expands to a real-world level, previous classifiers aligned with\\ncoarse class names significantly reduce the recognition performance of these\\ndetectors. In this paper, we introduce Prova, a multi-modal prototype\\nclassifier for vast-vocabulary object detection. Prova extracts comprehensive\\nmulti-modal prototypes as initialization of alignment classifiers to tackle the\\nvast-vocabulary object recognition failure problem. On V3Det, this simple\\nmethod greatly enhances the performance among one-stage, two-stage, and\\nDETR-based detectors with only additional projection layers in both supervised\\nand open-vocabulary settings. In particular, Prova improves Faster R-CNN, FCOS,\\nand DINO by 3.3, 6.2, and 2.9 AP respectively in the supervised setting of\\nV3Det. For the open-vocabulary setting, Prova achieves a new state-of-the-art\\nperformance with 32.8 base AP and 11.0 novel AP, which is of 2.6 and 4.3 gain\\nover the previous methods.\\nPublished: 2024-12-23 18:57:43+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV\\nPDF URL: http://arxiv.org/pdf/2412.17800v1\\narXiv URL: http://arxiv.org/abs/2412.17800v1\\n', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f443fb83-0549-4c0a-8b7e-45c956ad14a2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Memory makes computation universal, remember?\\nAuthors: Erik Garrison\\nSummary: Recent breakthroughs in AI capability have been attributed to increasingly\\nsophisticated architectures and alignment techniques, but a simpler principle\\nmay explain these advances: memory makes computation universal. Memory enables\\nuniversal computation through two fundamental capabilities: recursive state\\nmaintenance and reliable history access. We formally prove these requirements\\nare both necessary and sufficient for universal computation. This principle\\nmanifests across scales, from cellular computation to neural networks to\\nlanguage models. Complex behavior emerges not from sophisticated processing\\nunits but from maintaining and accessing state across time. We demonstrate how\\nparallel systems like neural networks achieve universal computation despite\\nlimitations in their basic units by maintaining state across iterations. This\\ntheoretical framework reveals a universal pattern: computational advances\\nconsistently emerge from enhanced abilities to maintain and access state rather\\nthan from more complex basic operations. Our analysis unifies understanding of\\ncomputation across biological systems, artificial intelligence, and human\\ncognition, reminding us that humanity's own computational capabilities have\\nevolved in step with our technical ability to remember through oral traditions,\\nwriting, and now computing.\\nPublished: 2024-12-23 18:51:46+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.LG\\nCategories: cs.LG\\nPDF URL: http://arxiv.org/pdf/2412.17794v1\\narXiv URL: http://arxiv.org/abs/2412.17794v1\\n\", mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='404a107f-6c07-46a1-ac7f-0294aed67e79', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Cross-Lingual Text-Rich Visual Comprehension: An Information Theory Perspective\\nAuthors: Xinmiao Yu, Xiaocheng Feng, Yun Li, Minghui Liao, Ya-Qi Yu, Xiachong Feng, Weihong Zhong, Ruihan Chen, Mengkang Hu, Jihao Wu, Dandan Tu, Duyu Tang, Bing Qin\\nSummary: Recent Large Vision-Language Models (LVLMs) have shown promising reasoning\\ncapabilities on text-rich images from charts, tables, and documents. However,\\nthe abundant text within such images may increase the model's sensitivity to\\nlanguage. This raises the need to evaluate LVLM performance on cross-lingual\\ntext-rich visual inputs, where the language in the image differs from the\\nlanguage of the instructions. To address this, we introduce XT-VQA\\n(Cross-Lingual Text-Rich Visual Question Answering), a benchmark designed to\\nassess how LVLMs handle language inconsistency between image text and\\nquestions. XT-VQA integrates five existing text-rich VQA datasets and a newly\\ncollected dataset, XPaperQA, covering diverse scenarios that require faithful\\nrecognition and comprehension of visual information despite language\\ninconsistency. Our evaluation of prominent LVLMs on XT-VQA reveals a\\nsignificant drop in performance for cross-lingual scenarios, even for models\\nwith multilingual capabilities. A mutual information analysis suggests that\\nthis performance gap stems from cross-lingual questions failing to adequately\\nactivate relevant visual information. To mitigate this issue, we propose\\nMVCL-MI (Maximization of Vision-Language Cross-Lingual Mutual Information),\\nwhere a visual-text cross-lingual alignment is built by maximizing mutual\\ninformation between the model's outputs and visual information. This is\\nachieved by distilling knowledge from monolingual to cross-lingual settings\\nthrough KL divergence minimization, where monolingual output logits serve as a\\nteacher. Experimental results on the XT-VQA demonstrate that MVCL-MI\\neffectively reduces the visual-text cross-lingual performance disparity while\\npreserving the inherent capabilities of LVLMs, shedding new light on the\\npotential practice for improving LVLMs. Codes are available at:\\nhttps://github.com/Stardust-y/XTVQA.git\\nPublished: 2024-12-23 18:48:04+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV, cs.CL\\nPDF URL: http://arxiv.org/pdf/2412.17787v1\\narXiv URL: http://arxiv.org/abs/2412.17787v1\\n\", mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='338fc350-1c5f-4af9-a64b-a8913492d78e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: PepTune: De Novo Generation of Therapeutic Peptides with Multi-Objective-Guided Discrete Diffusion\\nAuthors: Sophia Tang, Yinuo Zhang, Pranam Chatterjee\\nSummary: Peptide therapeutics, a major class of medicines, have achieved remarkable\\nsuccess across diseases such as diabetes and cancer, with landmark examples\\nsuch as GLP-1 receptor agonists revolutionizing the treatment of type-2\\ndiabetes and obesity. Despite their success, designing peptides that satisfy\\nmultiple conflicting objectives, such as target binding affinity, solubility,\\nand membrane permeability, remains a major challenge. Classical drug\\ndevelopment and structure-based design are ineffective for such tasks, as they\\nfail to optimize global functional properties critical for therapeutic\\nefficacy. Existing generative frameworks are largely limited to continuous\\nspaces, unconditioned outputs, or single-objective guidance, making them\\nunsuitable for discrete sequence optimization across multiple properties. To\\naddress this, we present PepTune, a multi-objective discrete diffusion model\\nfor the simultaneous generation and optimization of therapeutic peptide SMILES.\\nBuilt on the Masked Discrete Language Model (MDLM) framework, PepTune ensures\\nvalid peptide structures with state-dependent masking schedules and\\npenalty-based objectives. To guide the diffusion process, we propose a Monte\\nCarlo Tree Search (MCTS)-based strategy that balances exploration and\\nexploitation to iteratively refine Pareto-optimal sequences. MCTS integrates\\nclassifier-based rewards with search-tree expansion, overcoming gradient\\nestimation challenges and data sparsity inherent to discrete spaces. Using\\nPepTune, we generate diverse, chemically-modified peptides optimized for\\nmultiple therapeutic properties, including target binding affinity, membrane\\npermeability, solubility, hemolysis, and non-fouling characteristics on various\\ndisease-relevant targets. In total, our results demonstrate that MCTS-guided\\ndiscrete diffusion is a powerful and modular approach for multi-objective\\nsequence design in discrete state spaces.\\nPublished: 2024-12-23 18:38:49+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: q-bio.BM\\nCategories: q-bio.BM, cs.AI\\nPDF URL: http://arxiv.org/pdf/2412.17780v1\\narXiv URL: http://arxiv.org/abs/2412.17780v1\\n', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='23d8e3ba-b4cd-4b94-9023-886598fe5d24', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: ResearchTown: Simulator of Human Research Community\\nAuthors: Haofei Yu, Zhaochen Hong, Zirui Cheng, Kunlun Zhu, Keyang Xuan, Jinwei Yao, Tao Feng, Jiaxuan You\\nSummary: Large Language Models (LLMs) have demonstrated remarkable potential in\\nscientific domains, yet a fundamental question remains unanswered: Can we\\nsimulate human research communities with LLMs? Addressing this question can\\ndeepen our understanding of the processes behind idea brainstorming and inspire\\nthe automatic discovery of novel scientific insights. In this work, we propose\\nResearchTown, a multi-agent framework for research community simulation. Within\\nthis framework, the human research community is simplified and modeled as an\\nagent-data graph, where researchers and papers are represented as agent-type\\nand data-type nodes, respectively, and connected based on their collaboration\\nrelationships. We also introduce TextGNN, a text-based inference framework that\\nmodels various research activities (e.g., paper reading, paper writing, and\\nreview writing) as special forms of a unified message-passing process on the\\nagent-data graph. To evaluate the quality of the research simulation, we\\npresent ResearchBench, a benchmark that uses a node-masking prediction task for\\nscalable and objective assessment based on similarity. Our experiments reveal\\nthree key findings: (1) ResearchTown can provide a realistic simulation of\\ncollaborative research activities, including paper writing and review writing;\\n(2) ResearchTown can maintain robust simulation with multiple researchers and\\ndiverse papers; (3) ResearchTown can generate interdisciplinary research ideas\\nthat potentially inspire novel research directions.\\nPublished: 2024-12-23 18:26:53+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL, cs.LG\\nPDF URL: http://arxiv.org/pdf/2412.17767v1\\narXiv URL: http://arxiv.org/abs/2412.17767v1\\n', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f5fb74ea-a300-4d45-9462-9df553dbff70', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy\\nAuthors: Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Bhargava Kumar, Amit Agarwal, Ishan Banerjee, Srikant Panda, Tejaswini Kumar\\nSummary: Multimodal learning, a rapidly evolving field in artificial intelligence,\\nseeks to construct more versatile and robust systems by integrating and\\nanalyzing diverse types of data, including text, images, audio, and video.\\nInspired by the human ability to assimilate information through many senses,\\nthis method enables applications such as text-to-video conversion, visual\\nquestion answering, and image captioning. Recent developments in datasets that\\nsupport multimodal language models (MLLMs) are highlighted in this overview.\\nLarge-scale multimodal datasets are essential because they allow for thorough\\ntesting and training of these models. With an emphasis on their contributions\\nto the discipline, the study examines a variety of datasets, including those\\nfor training, domain-specific tasks, and real-world applications. It also\\nemphasizes how crucial benchmark datasets are for assessing models' performance\\nin a range of scenarios, scalability, and applicability. Since multimodal\\nlearning is always changing, overcoming these obstacles will help AI research\\nand applications reach new heights.\\nPublished: 2024-12-23 18:15:19+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.AI\\nCategories: cs.AI, cs.CV, cs.LG\\nPDF URL: http://arxiv.org/pdf/2412.17759v1\\narXiv URL: http://arxiv.org/abs/2412.17759v1\\n\", mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6418a318-0c26-4770-bf1d-c68ab49c7448', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: ADC: Enhancing Function Calling Via Adversarial Datasets and Code Line-Level Feedback\\nAuthors: Wei Zhang, Yi Zhang, Li Zhu, Qianghuai Jia, Feijun Jiang, Hongcheng Guo, Zhoujun Li, Mengping Zhou\\nSummary: Large Language Models (LLMs) have made significant strides in Natural\\nLanguage Processing and coding, yet they struggle with robustness and accuracy\\nin complex function calls. To tackle these challenges, this paper introduces\\nADC, an innovative approach that enhances LLMs' ability to follow function\\nformats and match complex parameters. ADC utilizes a high-quality code\\nfine-tuning dataset with line-level execution feedback, providing granular\\nprocess supervision that fosters strong logical reasoning and adherence to\\nfunction formats. It also employs an adversarial dataset generation process to\\nimprove parameter matching. The staged training methodology capitalizes on both\\nenriched code datasets and refined adversarial datasets, leading to marked\\nimprovements in function calling capabilities on the Berkeley Function-Calling\\nLeaderboard (BFCL) Benchmark. The innovation of ADC lies in its strategic\\ncombination of process supervision, adversarial refinement, and incremental\\nlearning, setting a new standard for LLM proficiency in complex function\\ncalling.\\nPublished: 2024-12-23 18:07:18+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.SE\\nCategories: cs.SE\\nPDF URL: http://arxiv.org/pdf/2412.17754v1\\narXiv URL: http://arxiv.org/abs/2412.17754v1\\n\", mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b121a0e1-4e3a-4552-8cbd-d8fb5143d72b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Deliberation in Latent Space via Differentiable Cache Augmentation\\nAuthors: Luyang Liu, Jonas Pfeiffer, Jiaxing Wu, Jun Xie, Arthur Szlam\\nSummary: Techniques enabling large language models (LLMs) to \"think more\" by\\ngenerating and attending to intermediate reasoning steps have shown promise in\\nsolving complex problems. However, the standard approaches generate sequences\\nof discrete tokens immediately before responding, and so they can incur\\nsignificant latency costs and be challenging to optimize. In this work, we\\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\\noperates on the model\\'s key-value (kv) cache. This coprocessor augments the\\ncache with a set of latent embeddings designed to improve the fidelity of\\nsubsequent decoding. We train this coprocessor using the language modeling loss\\nfrom the decoder on standard pretraining data, while keeping the decoder itself\\nfrozen. This approach enables the model to learn, in an end-to-end\\ndifferentiable fashion, how to distill additional computation into its\\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\\noffline and asynchronously, and the language model can function normally if the\\ncoprocessor is unavailable or if a given cache is deemed not to require extra\\ncomputation. We show experimentally that when a cache is augmented, the decoder\\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\\nwithout any task-specific training, our experiments demonstrate that cache\\naugmentation consistently reduces perplexity and improves performance across a\\nrange of reasoning-intensive tasks.\\nPublished: 2024-12-23 18:02:25+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL, cs.AI, cs.LG\\nPDF URL: http://arxiv.org/pdf/2412.17747v1\\narXiv URL: http://arxiv.org/abs/2412.17747v1\\n', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "\n",
    "from constants import embed_model\n",
    "\n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(\"index/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
